{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lgHHvGa2mjz3"
      },
      "source": [
        "## 1. Check GPU"
      ],
      "id": "lgHHvGa2mjz3"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ucgj0t77mjz9"
      },
      "source": [
        "## 1. Setup - Clone Repo & Install Dependencies"
      ],
      "id": "ucgj0t77mjz9"
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "rzzb71cOmjz-",
        "outputId": "dd158a0c-948c-4bb7-c45e-3af312c71085"
      },
      "source": [
        "# Install dependencies first\n",
        "%uv pip install -q tiktoken transformers datasets tqdm torch\n",
        "\n",
        "# Now import and setup\n",
        "import os\n",
        "import sys\n",
        "\n",
        "# Clone repo if running on Colab or Lightning AI (where model.py might be missing)\n",
        "if not os.path.exists('model.py'):\n",
        "    print(\"Cloning repository...\")\n",
        "    !git clone https://github.com/RealAndrewRen/nanoGPT.git\n",
        "    # Add the cloned repo to the python path so imports work\n",
        "    sys.path.append(os.path.abspath('nanoGPT'))\n",
        "    # Also change directory to the repo so relative paths work\n",
        "    os.chdir('nanoGPT')\n",
        "\n",
        "import tiktoken\n",
        "import transformers\n",
        "import datasets\n",
        "import tqdm\n",
        "print(f\"Current working directory: {os.getcwd()}\")\n",
        "print(\"‚úÖ Setup complete!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Cloning repository...\n",
            "fatal: destination path 'nanoGPT' already exists and is not an empty directory.\r\n",
            "Current working directory: /root/nanoGPT\n",
            "‚úÖ Setup complete!\n"
          ]
        }
      ],
      "id": "rzzb71cOmjz-"
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "GHUlUjcKmj0C",
        "outputId": "17dea515-555b-4918-a09c-726d2cc8c348"
      },
      "source": [
        "# Check GPU\n",
        "!nvidia-smi\n",
        "\n",
        "import torch\n",
        "print(f\"\\nPyTorch: {torch.__version__}\")\n",
        "print(f\"CUDA: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thu Dec  4 07:13:25 2025       \r\n",
            "+-----------------------------------------------------------------------------------------+\r\n",
            "| NVIDIA-SMI 575.57.08              Driver Version: 575.57.08      CUDA Version: 12.9     |\r\n",
            "|-----------------------------------------+------------------------+----------------------+\r\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\r\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\r\n",
            "|                                         |                        |               MIG M. |\r\n",
            "|=========================================+========================+======================|\r\n",
            "|   0  NVIDIA H100 NVL                On  |   00000001:00:00.0 Off |                    0 |\r\n",
            "| N/A   45C    P0             63W /  400W |       0MiB /  95830MiB |      0%      Default |\r\n",
            "|                                         |                        |             Disabled |\r\n",
            "+-----------------------------------------+------------------------+----------------------+\r\n",
            "                                                                                         \r\n",
            "+-----------------------------------------------------------------------------------------+\r\n",
            "| Processes:                                                                              |\r\n",
            "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\r\n",
            "|        ID   ID                                                               Usage      |\r\n",
            "|=========================================================================================|\r\n",
            "|  No running processes found                                                             |\r\n",
            "+-----------------------------------------------------------------------------------------+\r\n",
            "\n",
            "PyTorch: 2.8.0+cu129\n",
            "CUDA: True\n",
            "GPU: NVIDIA H100 NVL\n"
          ]
        }
      ],
      "id": "GHUlUjcKmj0C"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bYAcG7Exmj0D"
      },
      "source": [
        "## 2. Create Folders for Your Files"
      ],
      "id": "bYAcG7Exmj0D"
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "XYgQv3Odmj0E",
        "outputId": "1fd92d84-7f76-471a-8d24-14911bffde6b"
      },
      "source": [
        "import os\n",
        "\n",
        "os.makedirs('sft/data/finance_data', exist_ok=True)\n",
        "os.makedirs('out-finance_data', exist_ok=True)\n",
        "os.makedirs('out-sft', exist_ok=True)\n",
        "\n",
        "print(\"üìÅ Folders created!\")\n",
        "print(\"\")\n",
        "print(\"Now upload your files:\")\n",
        "print(\"  sft/data/finance_data/  ‚Üê train.bin, train_mask.bin, val.bin, val_mask.bin\")\n",
        "print(\"  out-finance_data/       ‚Üê ckpt.pt (Pretrained model)\")\n",
        "print(\"\")\n",
        "print(\"Use the file browser on the left or drag-and-drop to upload files.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìÅ Folders created!\n",
            "\n",
            "Now upload your files:\n",
            "  sft/data/finance_data/  ‚Üê train.bin, train_mask.bin, val.bin, val_mask.bin\n",
            "  out-finance_data/       ‚Üê ckpt.pt (Pretrained model)\n",
            "\n",
            "Use the file browser on the left or drag-and-drop to upload files.\n"
          ]
        }
      ],
      "id": "XYgQv3Odmj0E"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7LmrC_Vvmj0F"
      },
      "source": [
        "## 3. Verify Files Are In Place"
      ],
      "id": "7LmrC_Vvmj0F"
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "3qDCMgOemj0F",
        "outputId": "e3862232-e62a-49e1-c5b3-682351618c45"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "\n",
        "sft_dir = 'sft/data/finance_data'\n",
        "files_needed = ['train.bin', 'train_mask.bin', 'val.bin', 'val_mask.bin']\n",
        "\n",
        "print(\"SFT Data:\")\n",
        "all_ok = True\n",
        "for f in files_needed:\n",
        "    path = os.path.join(sft_dir, f)\n",
        "    if os.path.exists(path):\n",
        "        size = os.path.getsize(path) / (1024*1024)\n",
        "        print(f\"  ‚úÖ {f} ({size:.2f} MB)\")\n",
        "    else:\n",
        "        print(f\"  ‚ùå MISSING: {f}\")\n",
        "        all_ok = False\n",
        "\n",
        "print(\"\\nPretrained Checkpoint:\")\n",
        "ckpt_locations = [\n",
        "    'out-finance_data/ckpt.pt',\n",
        "    'out-finance-char/ckpt.pt'\n",
        "]\n",
        "\n",
        "ckpt_found = False\n",
        "for loc in ckpt_locations:\n",
        "    if os.path.exists(loc):\n",
        "        size = os.path.getsize(loc) / (1024*1024)\n",
        "        print(f\"  ‚úÖ {loc} ({size:.2f} MB)\")\n",
        "        ckpt_found = True\n",
        "        break\n",
        "\n",
        "if not ckpt_found:\n",
        "    print(f\"  ‚ùå MISSING: ckpt.pt (expected in out-finance_data/ or out-finance-char/)\")\n",
        "    all_ok = False\n",
        "\n",
        "if all_ok and os.path.exists(os.path.join(sft_dir, 'train.bin')):\n",
        "    train_tokens = np.fromfile(os.path.join(sft_dir, 'train.bin'), dtype=np.uint16)\n",
        "    train_mask = np.fromfile(os.path.join(sft_dir, 'train_mask.bin'), dtype=np.uint8)\n",
        "    print(f\"\\nüìä Dataset Statistics:\")\n",
        "    print(f\"Train tokens: {len(train_tokens):,}\")\n",
        "    print(f\"Mask coverage: {train_mask.sum() / len(train_mask) * 100:.1f}% assistant tokens\")\n",
        "\n",
        "    # Quick check for [CLEANED] tokens\n",
        "    import tiktoken\n",
        "    enc = tiktoken.get_encoding(\"gpt2\")\n",
        "    sample = train_tokens[:10000]\n",
        "    text = enc.decode(sample.tolist())\n",
        "    cleaned_count = text.count('[CLEANED]')\n",
        "\n",
        "    if cleaned_count > 0:\n",
        "        print(f\"\\n‚ö†Ô∏è WARNING: Found {cleaned_count} [CLEANED] tokens in sample!\")\n",
        "        print(\"Your training data may be corrupted. Consider regenerating with prepare_clean.py\")\n",
        "    else:\n",
        "        print(\"\\n‚úÖ Data looks clean! Ready to train!\")\n",
        "else:\n",
        "    print(\"\\n‚ö†Ô∏è Please upload the missing files before training.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SFT Data:\n",
            "  ‚úÖ train.bin (136.63 MB)\n",
            "  ‚úÖ train_mask.bin (68.31 MB)\n",
            "  ‚úÖ val.bin (15.27 MB)\n",
            "  ‚úÖ val_mask.bin (7.64 MB)\n",
            "\n",
            "Pretrained Checkpoint:\n",
            "  ‚úÖ out-finance_data/ckpt.pt (1417.50 MB)\n",
            "\n",
            "üìä Dataset Statistics:\n",
            "Train tokens: 71,632,581\n",
            "Mask coverage: 50.3% assistant tokens\n",
            "\n",
            "‚úÖ Data looks clean! Ready to train!\n"
          ]
        }
      ],
      "id": "3qDCMgOemj0F"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KManmImTmj0G"
      },
      "source": [
        "## 4. Training Configuration"
      ],
      "id": "KManmImTmj0G"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UcRTKAGNmj0H"
      },
      "source": [
        "## 3.5 Upload Your Files\n",
        "\n",
        "Upload these files to their respective directories:\n",
        "\n",
        "**SFT Data** ‚Üí `sft/data/finance_data/`:\n",
        "- `train.bin`\n",
        "- `train_mask.bin`\n",
        "- `val.bin`\n",
        "- `val_mask.bin`\n",
        "\n",
        "**Pretrained Model** ‚Üí `out-finance_data/`:\n",
        "- `ckpt.pt` (your pretrained checkpoint)\n",
        "\n",
        "You can upload files using the file browser in the sidebar or drag-and-drop."
      ],
      "id": "UcRTKAGNmj0H"
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "f1PdkI0zmj0I",
        "outputId": "2bcca167-967c-4123-e5cc-93c2959d7ce9"
      },
      "source": [
        "# ============================================================\n",
        "# SFT TRAINING CONFIG - Optimized for H100 GPU\n",
        "# ============================================================\n",
        "import os\n",
        "\n",
        "PRETRAINED_CKPT = 'out-finance_data/ckpt.pt'\n",
        "SFT_DATA_DIR = 'sft/data/finance_data'\n",
        "OUTPUT_DIR = 'out-sft'\n",
        "\n",
        "# H100 Settings (80GB VRAM - maximize utilization!)\n",
        "BATCH_SIZE = 64        # Increased to 64 to fully utilize H100\n",
        "GRADIENT_ACCUM = 2     # Effective batch size of 128 (64 * 2)\n",
        "BLOCK_SIZE = 256       # MUST match pretrained model's block_size!\n",
        "TARGET_EPOCHS = 2      # Target number of epochs (1-3 is standard for SFT)\n",
        "\n",
        "EVAL_INTERVAL = 250\n",
        "LOG_INTERVAL = 10\n",
        "\n",
        "LEARNING_RATE = 5e-6\n",
        "MIN_LR = 1e-6\n",
        "WARMUP_ITERS = 100\n",
        "\n",
        "DROPOUT = 0.1\n",
        "WEIGHT_DECAY = 0.1\n",
        "\n",
        "COMPILE = True         # Enable torch.compile for H100 (massive speedup)\n",
        "\n",
        "# Calculate MAX_ITERS based on dataset size\n",
        "tokens_per_step = BATCH_SIZE * GRADIENT_ACCUM * BLOCK_SIZE\n",
        "train_bin_path = os.path.join(SFT_DATA_DIR, 'train.bin')\n",
        "\n",
        "if os.path.exists(train_bin_path):\n",
        "    # train.bin is uint16 (2 bytes per token)\n",
        "    total_tokens = os.path.getsize(train_bin_path) // 2\n",
        "    steps_per_epoch = total_tokens // tokens_per_step\n",
        "    MAX_ITERS = steps_per_epoch * TARGET_EPOCHS\n",
        "    print(f\"Dataset size: {total_tokens/1e6:.1f}M tokens\")\n",
        "    print(f\"Steps per epoch: {steps_per_epoch}\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è train.bin not found, using default MAX_ITERS\")\n",
        "    MAX_ITERS = 5000 # Fallback\n",
        "\n",
        "print(f\"Effective batch size: {tokens_per_step:,} tokens\")\n",
        "print(f\"Learning rate: {LEARNING_RATE}\")\n",
        "print(f\"Max iterations: {MAX_ITERS} (Target: {TARGET_EPOCHS} epochs)\")\n",
        "print(f\"Compilation: {'Enabled' if COMPILE else 'Disabled'}\")\n",
        "print(f\"üöÄ H100 GPU detected - optimized for maximum throughput!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset size: 71.6M tokens\n",
            "Steps per epoch: 2186\n",
            "Effective batch size: 32,768 tokens\n",
            "Learning rate: 5e-06\n",
            "Max iterations: 4372 (Target: 2 epochs)\n",
            "Compilation: Enabled\n",
            "üöÄ H100 GPU detected - optimized for maximum throughput!\n"
          ]
        }
      ],
      "id": "f1PdkI0zmj0I"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9gg2LWAdmj0I"
      },
      "source": [
        "## 5. Run SFT Training"
      ],
      "id": "9gg2LWAdmj0I"
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "KRJtgjbymj0J",
        "outputId": "95ccb5a8-9e9d-4d39-a9be-0eb03db3af66"
      },
      "source": [
        "import os\n",
        "import time\n",
        "import math\n",
        "from contextlib import nullcontext\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from model import GPTConfig, GPT\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "dtype = 'bfloat16' if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else 'float16'\n",
        "ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]\n",
        "ctx = torch.amp.autocast(device_type='cuda', dtype=ptdtype) if device == 'cuda' else nullcontext()\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "print(f\"Device: {device}, Dtype: {dtype}\")\n",
        "print(f\"Block size: {BLOCK_SIZE}\")\n",
        "\n",
        "# Data loader - FIXED with validation\n",
        "def get_batch(split):\n",
        "    if split == 'train':\n",
        "        data = np.memmap(os.path.join(SFT_DATA_DIR, 'train.bin'), dtype=np.uint16, mode='r')\n",
        "        mask = np.memmap(os.path.join(SFT_DATA_DIR, 'train_mask.bin'), dtype=np.uint8, mode='r')\n",
        "    else:\n",
        "        data = np.memmap(os.path.join(SFT_DATA_DIR, 'val.bin'), dtype=np.uint16, mode='r')\n",
        "        mask = np.memmap(os.path.join(SFT_DATA_DIR, 'val_mask.bin'), dtype=np.uint8, mode='r')\n",
        "\n",
        "    # Ensure we have enough data\n",
        "    if len(data) <= BLOCK_SIZE:\n",
        "        raise ValueError(f\"Dataset too small! Has {len(data)} tokens, need at least {BLOCK_SIZE + 1}\")\n",
        "\n",
        "    # Generate random starting indices - ensure they don't exceed bounds\n",
        "    max_start = len(data) - BLOCK_SIZE - 1\n",
        "    ix = torch.randint(0, max_start, (BATCH_SIZE,))\n",
        "\n",
        "    # Build batches with explicit size control\n",
        "    x = torch.stack([torch.from_numpy(data[i:i+BLOCK_SIZE].astype(np.int64)) for i in ix])\n",
        "    y = torch.stack([torch.from_numpy(data[i+1:i+1+BLOCK_SIZE].astype(np.int64)) for i in ix])\n",
        "    m = torch.stack([torch.from_numpy(mask[i+1:i+1+BLOCK_SIZE].astype(np.float32)) for i in ix])\n",
        "\n",
        "    # Validate shapes\n",
        "    assert x.shape == (BATCH_SIZE, BLOCK_SIZE), f\"X shape mismatch: {x.shape}\"\n",
        "    assert y.shape == (BATCH_SIZE, BLOCK_SIZE), f\"Y shape mismatch: {y.shape}\"\n",
        "    assert m.shape == (BATCH_SIZE, BLOCK_SIZE), f\"M shape mismatch: {m.shape}\"\n",
        "\n",
        "    return x.to(device), y.to(device), m.to(device)\n",
        "\n",
        "# Load pretrained model\n",
        "print(f\"\\nLoading pretrained model from {PRETRAINED_CKPT}...\")\n",
        "checkpoint = torch.load(PRETRAINED_CKPT, map_location=device)\n",
        "model_args = checkpoint['model_args']\n",
        "model_args['dropout'] = DROPOUT\n",
        "\n",
        "gptconf = GPTConfig(**model_args)\n",
        "model = GPT(gptconf)\n",
        "\n",
        "state_dict = checkpoint['model']\n",
        "unwanted_prefix = '_orig_mod.'\n",
        "for k, v in list(state_dict.items()):\n",
        "    if k.startswith(unwanted_prefix):\n",
        "        state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)\n",
        "\n",
        "model.load_state_dict(state_dict)\n",
        "model.to(device)\n",
        "print(f\"Model loaded! Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
        "print(f\"Model block size: {model.config.block_size}\")\n",
        "\n",
        "optimizer = model.configure_optimizers(WEIGHT_DECAY, LEARNING_RATE, (0.9, 0.95), 'cuda')\n",
        "scaler = torch.amp.GradScaler('cuda', enabled=(dtype == 'float16'))\n",
        "\n",
        "# Masked loss function\n",
        "def compute_masked_loss(logits, targets, mask):\n",
        "    B, T, V = logits.shape\n",
        "    loss_per_token = F.cross_entropy(\n",
        "        logits.view(B * T, V),\n",
        "        targets.view(B * T),\n",
        "        reduction='none'\n",
        "    ).view(B, T)\n",
        "\n",
        "    masked_loss = loss_per_token * mask\n",
        "    num_masked = mask.sum()\n",
        "\n",
        "    if num_masked > 0:\n",
        "        return masked_loss.sum() / num_masked\n",
        "    return loss_per_token.mean()\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss(eval_iters=50):\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y, M = get_batch(split)\n",
        "            with ctx:\n",
        "                logits, _ = model(X)  # Model returns (logits, None) when no targets\n",
        "                loss = compute_masked_loss(logits, Y, M)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "def get_lr(it):\n",
        "    if it < WARMUP_ITERS:\n",
        "        return LEARNING_RATE * (it + 1) / (WARMUP_ITERS + 1)\n",
        "    if it > MAX_ITERS:\n",
        "        return MIN_LR\n",
        "    decay_ratio = (it - WARMUP_ITERS) / (MAX_ITERS - WARMUP_ITERS)\n",
        "    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio))\n",
        "    return MIN_LR + coeff * (LEARNING_RATE - MIN_LR)\n",
        "\n",
        "print(f\"\\n{'='*50}\")\n",
        "print(\"STARTING SFT TRAINING\")\n",
        "print(f\"{'='*50}\")\n",
        "\n",
        "iter_num = 0\n",
        "best_val_loss = 1e9\n",
        "t0 = time.time()\n",
        "X, Y, M = get_batch('train')\n",
        "\n",
        "# Compile model after first eval (avoids compilation issues)\n",
        "compiled = False\n",
        "\n",
        "while iter_num <= MAX_ITERS:\n",
        "    lr = get_lr(iter_num)\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = lr\n",
        "\n",
        "    if iter_num % EVAL_INTERVAL == 0:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"\\nStep {iter_num}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "        # Compile after first successful eval\n",
        "        if not compiled and COMPILE and iter_num > 0:\n",
        "            print(\"Compiling model for faster training...\")\n",
        "            model = torch.compile(model)\n",
        "            compiled = True\n",
        "\n",
        "        if losses['val'] < best_val_loss:\n",
        "            best_val_loss = losses['val']\n",
        "            if iter_num > 0:\n",
        "                ckpt = {\n",
        "                    'model': model._orig_mod.state_dict() if compiled else model.state_dict(),\n",
        "                    'optimizer': optimizer.state_dict(),\n",
        "                    'model_args': model_args,\n",
        "                    'iter_num': iter_num,\n",
        "                    'best_val_loss': best_val_loss,\n",
        "                }\n",
        "                torch.save(ckpt, os.path.join(OUTPUT_DIR, 'ckpt.pt'))\n",
        "                print(f\"üíæ Saved checkpoint (val_loss: {best_val_loss:.4f})\")\n",
        "\n",
        "    for micro_step in range(GRADIENT_ACCUM):\n",
        "        with ctx:\n",
        "            logits, _ = model(X)  # Model returns (logits, None) when no targets\n",
        "            loss = compute_masked_loss(logits, Y, M) / GRADIENT_ACCUM\n",
        "        X, Y, M = get_batch('train')\n",
        "        scaler.scale(loss).backward()\n",
        "\n",
        "    scaler.unscale_(optimizer)\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "    scaler.step(optimizer)\n",
        "    scaler.update()\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "    t1 = time.time()\n",
        "    dt = t1 - t0\n",
        "    t0 = t1\n",
        "\n",
        "    if iter_num % LOG_INTERVAL == 0:\n",
        "        lossf = loss.item() * GRADIENT_ACCUM\n",
        "        print(f\"iter {iter_num}: loss {lossf:.4f}, lr {lr:.2e}, time {dt*1000:.0f}ms\")\n",
        "\n",
        "    iter_num += 1\n",
        "\n",
        "print(f\"\\n{'='*50}\")\n",
        "print(f\"‚úÖ TRAINING COMPLETE! Best val loss: {best_val_loss:.4f}\")\n",
        "print(f\"Model saved to: {OUTPUT_DIR}/ckpt.pt\")\n",
        "print(f\"{'='*50}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda, Dtype: bfloat16\n",
            "Block size: 256\n",
            "\n",
            "Loading pretrained model from out-finance_data/ckpt.pt...\n",
            "number of parameters: 123.65M\n",
            "Model loaded! Parameters: 123,849,984\n",
            "Model block size: 256\n",
            "num decayed parameter tensors: 50, with 123,728,640 parameters\n",
            "num non-decayed parameter tensors: 98, with 121,344 parameters\n",
            "using fused AdamW: True\n",
            "\n",
            "==================================================\n",
            "STARTING SFT TRAINING\n",
            "==================================================\n",
            "\n",
            "Step 0: train loss 4.7159, val loss 4.7650\n",
            "iter 0: loss 4.7136, lr 4.95e-08, time 3849ms\n",
            "iter 10: loss 4.8235, lr 5.45e-07, time 180ms\n",
            "iter 20: loss 4.4692, lr 1.04e-06, time 179ms\n",
            "iter 30: loss 4.3630, lr 1.53e-06, time 180ms\n",
            "iter 40: loss 4.4721, lr 2.03e-06, time 180ms\n",
            "iter 50: loss 4.0233, lr 2.52e-06, time 181ms\n",
            "iter 60: loss 3.5951, lr 3.02e-06, time 179ms\n",
            "iter 70: loss 3.6870, lr 3.51e-06, time 180ms\n",
            "iter 80: loss 3.5237, lr 4.01e-06, time 180ms\n",
            "iter 90: loss 3.4842, lr 4.50e-06, time 179ms\n",
            "iter 100: loss 3.4339, lr 5.00e-06, time 181ms\n",
            "iter 110: loss 3.1898, lr 5.00e-06, time 181ms\n",
            "iter 120: loss 3.1866, lr 5.00e-06, time 181ms\n",
            "iter 130: loss 3.2847, lr 5.00e-06, time 182ms\n",
            "iter 140: loss 3.0892, lr 5.00e-06, time 180ms\n",
            "iter 150: loss 3.1271, lr 5.00e-06, time 180ms\n",
            "iter 160: loss 3.0973, lr 5.00e-06, time 179ms\n",
            "iter 170: loss 3.0127, lr 5.00e-06, time 182ms\n",
            "iter 180: loss 3.1490, lr 5.00e-06, time 184ms\n",
            "iter 190: loss 3.1141, lr 5.00e-06, time 179ms\n",
            "iter 200: loss 2.9854, lr 4.99e-06, time 180ms\n",
            "iter 210: loss 2.8875, lr 4.99e-06, time 182ms\n",
            "iter 220: loss 2.9418, lr 4.99e-06, time 182ms\n",
            "iter 230: loss 2.9160, lr 4.99e-06, time 182ms\n",
            "iter 240: loss 2.9915, lr 4.99e-06, time 181ms\n",
            "\n",
            "Step 250: train loss 2.8238, val loss 2.8362\n",
            "Compiling model for faster training...\n",
            "üíæ Saved checkpoint (val_loss: 2.8362)\n",
            "iter 250: loss 2.9009, lr 4.99e-06, time 44289ms\n",
            "iter 260: loss 2.9389, lr 4.99e-06, time 126ms\n",
            "iter 270: loss 3.0542, lr 4.98e-06, time 129ms\n",
            "iter 280: loss 2.9236, lr 4.98e-06, time 130ms\n",
            "iter 290: loss 2.9182, lr 4.98e-06, time 129ms\n",
            "iter 300: loss 2.8562, lr 4.98e-06, time 130ms\n",
            "iter 310: loss 2.9210, lr 4.98e-06, time 128ms\n",
            "iter 320: loss 3.0855, lr 4.97e-06, time 129ms\n",
            "iter 330: loss 2.7754, lr 4.97e-06, time 131ms\n",
            "iter 340: loss 3.0137, lr 4.97e-06, time 129ms\n",
            "iter 350: loss 3.0187, lr 4.97e-06, time 130ms\n",
            "iter 360: loss 2.9713, lr 4.96e-06, time 129ms\n",
            "iter 370: loss 2.7814, lr 4.96e-06, time 129ms\n",
            "iter 380: loss 2.8973, lr 4.96e-06, time 131ms\n",
            "iter 390: loss 2.6888, lr 4.95e-06, time 129ms\n",
            "iter 400: loss 2.6599, lr 4.95e-06, time 129ms\n",
            "iter 410: loss 2.7685, lr 4.95e-06, time 130ms\n",
            "iter 420: loss 2.8835, lr 4.94e-06, time 130ms\n",
            "iter 430: loss 2.7348, lr 4.94e-06, time 129ms\n",
            "iter 440: loss 2.7727, lr 4.94e-06, time 133ms\n",
            "iter 450: loss 2.8085, lr 4.93e-06, time 131ms\n",
            "iter 460: loss 2.7986, lr 4.93e-06, time 131ms\n",
            "iter 470: loss 2.4832, lr 4.93e-06, time 131ms\n",
            "iter 480: loss 2.7930, lr 4.92e-06, time 132ms\n",
            "iter 490: loss 2.5920, lr 4.92e-06, time 132ms\n",
            "\n",
            "Step 500: train loss 2.6491, val loss 2.6706\n",
            "üíæ Saved checkpoint (val_loss: 2.6706)\n",
            "iter 500: loss 2.7826, lr 4.91e-06, time 20599ms\n",
            "iter 510: loss 2.5888, lr 4.91e-06, time 128ms\n",
            "iter 520: loss 2.6760, lr 4.91e-06, time 132ms\n",
            "iter 530: loss 2.6550, lr 4.90e-06, time 128ms\n",
            "iter 540: loss 2.6707, lr 4.90e-06, time 133ms\n",
            "iter 550: loss 2.6776, lr 4.89e-06, time 131ms\n",
            "iter 560: loss 2.5952, lr 4.89e-06, time 134ms\n",
            "iter 570: loss 2.7921, lr 4.88e-06, time 130ms\n",
            "iter 580: loss 2.7717, lr 4.88e-06, time 129ms\n",
            "iter 590: loss 2.6892, lr 4.87e-06, time 131ms\n",
            "iter 600: loss 2.6134, lr 4.87e-06, time 133ms\n",
            "iter 610: loss 2.5711, lr 4.86e-06, time 133ms\n",
            "iter 620: loss 2.6441, lr 4.86e-06, time 133ms\n",
            "iter 630: loss 2.7375, lr 4.85e-06, time 132ms\n",
            "iter 640: loss 2.6602, lr 4.84e-06, time 133ms\n",
            "iter 650: loss 2.7890, lr 4.84e-06, time 133ms\n",
            "iter 660: loss 2.5672, lr 4.83e-06, time 132ms\n",
            "iter 670: loss 2.7287, lr 4.83e-06, time 132ms\n",
            "iter 680: loss 2.9242, lr 4.82e-06, time 134ms\n",
            "iter 690: loss 2.6385, lr 4.81e-06, time 134ms\n",
            "iter 700: loss 2.7732, lr 4.81e-06, time 132ms\n",
            "iter 710: loss 2.6955, lr 4.80e-06, time 131ms\n",
            "iter 720: loss 2.5387, lr 4.80e-06, time 131ms\n",
            "iter 730: loss 2.7193, lr 4.79e-06, time 130ms\n",
            "iter 740: loss 2.6121, lr 4.78e-06, time 134ms\n",
            "\n",
            "Step 750: train loss 2.5320, val loss 2.5653\n",
            "üíæ Saved checkpoint (val_loss: 2.5653)\n",
            "iter 750: loss 2.7043, lr 4.78e-06, time 3720ms\n",
            "iter 760: loss 2.5333, lr 4.77e-06, time 132ms\n",
            "iter 770: loss 2.5670, lr 4.76e-06, time 132ms\n",
            "iter 780: loss 2.6766, lr 4.76e-06, time 134ms\n",
            "iter 790: loss 2.5791, lr 4.75e-06, time 133ms\n",
            "iter 800: loss 2.7204, lr 4.74e-06, time 132ms\n",
            "iter 810: loss 2.7246, lr 4.73e-06, time 133ms\n",
            "iter 820: loss 2.6012, lr 4.73e-06, time 133ms\n",
            "iter 830: loss 2.6179, lr 4.72e-06, time 133ms\n",
            "iter 840: loss 2.5195, lr 4.71e-06, time 134ms\n",
            "iter 850: loss 2.5795, lr 4.70e-06, time 134ms\n",
            "iter 860: loss 2.6457, lr 4.70e-06, time 133ms\n",
            "iter 870: loss 2.8628, lr 4.69e-06, time 134ms\n",
            "iter 880: loss 2.4100, lr 4.68e-06, time 133ms\n",
            "iter 890: loss 2.6356, lr 4.67e-06, time 133ms\n",
            "iter 900: loss 2.6656, lr 4.66e-06, time 135ms\n",
            "iter 910: loss 2.7178, lr 4.66e-06, time 134ms\n",
            "iter 920: loss 2.6335, lr 4.65e-06, time 135ms\n",
            "iter 930: loss 2.4914, lr 4.64e-06, time 133ms\n",
            "iter 940: loss 2.6599, lr 4.63e-06, time 132ms\n",
            "iter 950: loss 2.8804, lr 4.62e-06, time 133ms\n",
            "iter 960: loss 2.7669, lr 4.61e-06, time 135ms\n",
            "iter 970: loss 2.5439, lr 4.60e-06, time 133ms\n",
            "iter 980: loss 2.4765, lr 4.60e-06, time 133ms\n",
            "iter 990: loss 2.4163, lr 4.59e-06, time 134ms\n",
            "\n",
            "Step 1000: train loss 2.4893, val loss 2.4989\n",
            "üíæ Saved checkpoint (val_loss: 2.4989)\n",
            "iter 1000: loss 2.6887, lr 4.58e-06, time 3715ms\n",
            "iter 1010: loss 2.5721, lr 4.57e-06, time 133ms\n",
            "iter 1020: loss 2.5628, lr 4.56e-06, time 133ms\n",
            "iter 1030: loss 2.5323, lr 4.55e-06, time 134ms\n",
            "iter 1040: loss 2.6050, lr 4.54e-06, time 133ms\n",
            "iter 1050: loss 2.4636, lr 4.53e-06, time 133ms\n",
            "iter 1060: loss 2.5920, lr 4.52e-06, time 132ms\n",
            "iter 1070: loss 2.6071, lr 4.51e-06, time 132ms\n",
            "iter 1080: loss 2.7362, lr 4.50e-06, time 134ms\n",
            "iter 1090: loss 2.5623, lr 4.49e-06, time 135ms\n",
            "iter 1100: loss 2.4715, lr 4.48e-06, time 133ms\n",
            "iter 1110: loss 2.5849, lr 4.47e-06, time 133ms\n",
            "iter 1120: loss 2.5837, lr 4.46e-06, time 133ms\n",
            "iter 1130: loss 2.6693, lr 4.45e-06, time 131ms\n",
            "iter 1140: loss 2.7221, lr 4.44e-06, time 134ms\n",
            "iter 1150: loss 2.5876, lr 4.43e-06, time 134ms\n",
            "iter 1160: loss 2.4747, lr 4.42e-06, time 134ms\n",
            "iter 1170: loss 2.4906, lr 4.41e-06, time 135ms\n",
            "iter 1180: loss 2.4235, lr 4.40e-06, time 132ms\n",
            "iter 1190: loss 2.5535, lr 4.39e-06, time 133ms\n",
            "iter 1200: loss 2.5241, lr 4.38e-06, time 133ms\n",
            "iter 1210: loss 2.3572, lr 4.37e-06, time 133ms\n",
            "iter 1220: loss 2.5185, lr 4.36e-06, time 133ms\n",
            "iter 1230: loss 2.3616, lr 4.35e-06, time 133ms\n",
            "iter 1240: loss 2.5804, lr 4.34e-06, time 133ms\n",
            "\n",
            "Step 1250: train loss 2.4621, val loss 2.4530\n",
            "üíæ Saved checkpoint (val_loss: 2.4530)\n",
            "iter 1250: loss 2.5672, lr 4.33e-06, time 3689ms\n",
            "iter 1260: loss 2.5642, lr 4.32e-06, time 133ms\n",
            "iter 1270: loss 2.6385, lr 4.30e-06, time 133ms\n",
            "iter 1280: loss 2.4635, lr 4.29e-06, time 133ms\n",
            "iter 1290: loss 2.4562, lr 4.28e-06, time 132ms\n",
            "iter 1300: loss 2.5454, lr 4.27e-06, time 133ms\n",
            "iter 1310: loss 2.4246, lr 4.26e-06, time 134ms\n",
            "iter 1320: loss 2.3959, lr 4.25e-06, time 133ms\n",
            "iter 1330: loss 2.6187, lr 4.24e-06, time 135ms\n",
            "iter 1340: loss 2.6511, lr 4.22e-06, time 133ms\n",
            "iter 1350: loss 2.5621, lr 4.21e-06, time 135ms\n",
            "iter 1360: loss 2.3993, lr 4.20e-06, time 133ms\n",
            "iter 1370: loss 2.7034, lr 4.19e-06, time 133ms\n",
            "iter 1380: loss 2.5945, lr 4.18e-06, time 133ms\n",
            "iter 1390: loss 2.7020, lr 4.17e-06, time 133ms\n",
            "iter 1400: loss 2.5994, lr 4.15e-06, time 134ms\n",
            "iter 1410: loss 2.5434, lr 4.14e-06, time 134ms\n",
            "iter 1420: loss 2.5307, lr 4.13e-06, time 134ms\n",
            "iter 1430: loss 2.4046, lr 4.12e-06, time 133ms\n",
            "iter 1440: loss 2.4354, lr 4.11e-06, time 133ms\n",
            "iter 1450: loss 2.4406, lr 4.09e-06, time 134ms\n",
            "iter 1460: loss 2.5672, lr 4.08e-06, time 133ms\n",
            "iter 1470: loss 2.4452, lr 4.07e-06, time 133ms\n",
            "iter 1480: loss 2.5909, lr 4.06e-06, time 133ms\n",
            "iter 1490: loss 2.5619, lr 4.04e-06, time 134ms\n",
            "\n",
            "Step 1500: train loss 2.4198, val loss 2.4225\n",
            "üíæ Saved checkpoint (val_loss: 2.4225)\n",
            "iter 1500: loss 2.5262, lr 4.03e-06, time 3703ms\n",
            "iter 1510: loss 2.3895, lr 4.02e-06, time 132ms\n",
            "iter 1520: loss 2.3324, lr 4.01e-06, time 134ms\n",
            "iter 1530: loss 2.7709, lr 3.99e-06, time 133ms\n",
            "iter 1540: loss 2.7709, lr 3.98e-06, time 134ms\n",
            "iter 1550: loss 2.6675, lr 3.97e-06, time 134ms\n",
            "iter 1560: loss 2.6110, lr 3.95e-06, time 131ms\n",
            "iter 1570: loss 2.4270, lr 3.94e-06, time 133ms\n",
            "iter 1580: loss 2.2600, lr 3.93e-06, time 133ms\n",
            "iter 1590: loss 2.5250, lr 3.91e-06, time 133ms\n",
            "iter 1600: loss 2.5831, lr 3.90e-06, time 132ms\n",
            "iter 1610: loss 2.5746, lr 3.89e-06, time 133ms\n",
            "iter 1620: loss 2.5482, lr 3.88e-06, time 133ms\n",
            "iter 1630: loss 2.3607, lr 3.86e-06, time 134ms\n",
            "iter 1640: loss 2.5948, lr 3.85e-06, time 134ms\n",
            "iter 1650: loss 2.6449, lr 3.84e-06, time 133ms\n",
            "iter 1660: loss 2.5919, lr 3.82e-06, time 135ms\n",
            "iter 1670: loss 2.3427, lr 3.81e-06, time 135ms\n",
            "iter 1680: loss 2.4975, lr 3.80e-06, time 132ms\n",
            "iter 1690: loss 2.4680, lr 3.78e-06, time 134ms\n",
            "iter 1700: loss 2.3785, lr 3.77e-06, time 133ms\n",
            "iter 1710: loss 2.5657, lr 3.75e-06, time 133ms\n",
            "iter 1720: loss 2.5321, lr 3.74e-06, time 133ms\n",
            "iter 1730: loss 2.3570, lr 3.73e-06, time 134ms\n",
            "iter 1740: loss 2.5310, lr 3.71e-06, time 132ms\n",
            "\n",
            "Step 1750: train loss 2.3916, val loss 2.3823\n",
            "üíæ Saved checkpoint (val_loss: 2.3823)\n",
            "iter 1750: loss 2.3993, lr 3.70e-06, time 3669ms\n",
            "iter 1760: loss 2.4250, lr 3.69e-06, time 131ms\n",
            "iter 1770: loss 2.4364, lr 3.67e-06, time 132ms\n",
            "iter 1780: loss 2.4638, lr 3.66e-06, time 132ms\n",
            "iter 1790: loss 2.6684, lr 3.64e-06, time 134ms\n",
            "iter 1800: loss 2.5625, lr 3.63e-06, time 134ms\n",
            "iter 1810: loss 2.3183, lr 3.62e-06, time 133ms\n",
            "iter 1820: loss 2.4304, lr 3.60e-06, time 132ms\n",
            "iter 1830: loss 2.5316, lr 3.59e-06, time 133ms\n",
            "iter 1840: loss 2.6313, lr 3.57e-06, time 133ms\n",
            "iter 1850: loss 2.3735, lr 3.56e-06, time 133ms\n",
            "iter 1860: loss 2.4140, lr 3.55e-06, time 133ms\n",
            "iter 1870: loss 2.5502, lr 3.53e-06, time 133ms\n",
            "iter 1880: loss 2.4825, lr 3.52e-06, time 132ms\n",
            "iter 1890: loss 2.4321, lr 3.50e-06, time 134ms\n",
            "iter 1900: loss 2.6053, lr 3.49e-06, time 133ms\n",
            "iter 1910: loss 2.5170, lr 3.47e-06, time 133ms\n",
            "iter 1920: loss 2.5330, lr 3.46e-06, time 134ms\n",
            "iter 1930: loss 2.4849, lr 3.45e-06, time 134ms\n",
            "iter 1940: loss 2.5562, lr 3.43e-06, time 133ms\n",
            "iter 1950: loss 2.3981, lr 3.42e-06, time 134ms\n",
            "iter 1960: loss 2.6858, lr 3.40e-06, time 134ms\n",
            "iter 1970: loss 2.6402, lr 3.39e-06, time 132ms\n",
            "iter 1980: loss 2.5035, lr 3.37e-06, time 132ms\n",
            "iter 1990: loss 2.4281, lr 3.36e-06, time 133ms\n",
            "\n",
            "Step 2000: train loss 2.3726, val loss 2.4228\n",
            "iter 2000: loss 2.5099, lr 3.35e-06, time 2493ms\n",
            "iter 2010: loss 2.5049, lr 3.33e-06, time 134ms\n",
            "iter 2020: loss 2.6159, lr 3.32e-06, time 133ms\n",
            "iter 2030: loss 2.4373, lr 3.30e-06, time 133ms\n",
            "iter 2040: loss 2.4006, lr 3.29e-06, time 133ms\n",
            "iter 2050: loss 2.6374, lr 3.27e-06, time 133ms\n",
            "iter 2060: loss 2.4972, lr 3.26e-06, time 132ms\n",
            "iter 2070: loss 2.4495, lr 3.24e-06, time 134ms\n",
            "iter 2080: loss 2.6403, lr 3.23e-06, time 132ms\n",
            "iter 2090: loss 2.5537, lr 3.21e-06, time 133ms\n",
            "iter 2100: loss 2.2833, lr 3.20e-06, time 132ms\n",
            "iter 2110: loss 2.1743, lr 3.19e-06, time 133ms\n",
            "iter 2120: loss 2.5973, lr 3.17e-06, time 131ms\n",
            "iter 2130: loss 2.4862, lr 3.16e-06, time 134ms\n",
            "iter 2140: loss 2.4198, lr 3.14e-06, time 133ms\n",
            "iter 2150: loss 2.1990, lr 3.13e-06, time 133ms\n",
            "iter 2160: loss 2.4583, lr 3.11e-06, time 132ms\n",
            "iter 2170: loss 2.6300, lr 3.10e-06, time 133ms\n",
            "iter 2180: loss 2.4592, lr 3.08e-06, time 134ms\n",
            "iter 2190: loss 2.4244, lr 3.07e-06, time 133ms\n",
            "iter 2200: loss 2.5618, lr 3.05e-06, time 134ms\n",
            "iter 2210: loss 2.4353, lr 3.04e-06, time 131ms\n",
            "iter 2220: loss 2.3466, lr 3.02e-06, time 134ms\n",
            "iter 2230: loss 2.4253, lr 3.01e-06, time 133ms\n",
            "iter 2240: loss 2.4848, lr 2.99e-06, time 132ms\n",
            "\n",
            "Step 2250: train loss 2.3929, val loss 2.3650\n",
            "üíæ Saved checkpoint (val_loss: 2.3650)\n",
            "iter 2250: loss 2.4592, lr 2.98e-06, time 3691ms\n",
            "iter 2260: loss 2.4190, lr 2.96e-06, time 135ms\n",
            "iter 2270: loss 2.3348, lr 2.95e-06, time 133ms\n",
            "iter 2280: loss 2.4371, lr 2.94e-06, time 133ms\n",
            "iter 2290: loss 2.5529, lr 2.92e-06, time 134ms\n",
            "iter 2300: loss 2.4817, lr 2.91e-06, time 133ms\n",
            "iter 2310: loss 2.4019, lr 2.89e-06, time 134ms\n",
            "iter 2320: loss 2.7086, lr 2.88e-06, time 133ms\n",
            "iter 2330: loss 2.6167, lr 2.86e-06, time 134ms\n",
            "iter 2340: loss 2.4738, lr 2.85e-06, time 134ms\n",
            "iter 2350: loss 2.5283, lr 2.83e-06, time 132ms\n",
            "iter 2360: loss 2.2944, lr 2.82e-06, time 133ms\n",
            "iter 2370: loss 2.4836, lr 2.80e-06, time 134ms\n",
            "iter 2380: loss 2.4073, lr 2.79e-06, time 134ms\n",
            "iter 2390: loss 2.5654, lr 2.77e-06, time 133ms\n",
            "iter 2400: loss 2.4290, lr 2.76e-06, time 133ms\n",
            "iter 2410: loss 2.3820, lr 2.74e-06, time 132ms\n",
            "iter 2420: loss 2.6159, lr 2.73e-06, time 133ms\n",
            "iter 2430: loss 2.3424, lr 2.72e-06, time 132ms\n",
            "iter 2440: loss 2.3072, lr 2.70e-06, time 134ms\n",
            "iter 2450: loss 2.5929, lr 2.69e-06, time 134ms\n",
            "iter 2460: loss 2.2482, lr 2.67e-06, time 134ms\n",
            "iter 2470: loss 2.5589, lr 2.66e-06, time 132ms\n",
            "iter 2480: loss 2.5693, lr 2.64e-06, time 135ms\n",
            "iter 2490: loss 2.5818, lr 2.63e-06, time 133ms\n",
            "\n",
            "Step 2500: train loss 2.3856, val loss 2.3789\n",
            "iter 2500: loss 2.4502, lr 2.61e-06, time 2495ms\n",
            "iter 2510: loss 2.5523, lr 2.60e-06, time 133ms\n",
            "iter 2520: loss 2.6773, lr 2.59e-06, time 134ms\n",
            "iter 2530: loss 2.5275, lr 2.57e-06, time 133ms\n",
            "iter 2540: loss 2.4498, lr 2.56e-06, time 132ms\n",
            "iter 2550: loss 2.4619, lr 2.54e-06, time 135ms\n",
            "iter 2560: loss 2.4886, lr 2.53e-06, time 132ms\n",
            "iter 2570: loss 2.3800, lr 2.51e-06, time 133ms\n",
            "iter 2580: loss 2.4321, lr 2.50e-06, time 134ms\n",
            "iter 2590: loss 2.5490, lr 2.49e-06, time 134ms\n",
            "iter 2600: loss 2.5173, lr 2.47e-06, time 133ms\n",
            "iter 2610: loss 2.4700, lr 2.46e-06, time 132ms\n",
            "iter 2620: loss 2.5226, lr 2.44e-06, time 133ms\n",
            "iter 2630: loss 2.5386, lr 2.43e-06, time 133ms\n",
            "iter 2640: loss 2.5250, lr 2.41e-06, time 131ms\n",
            "iter 2650: loss 2.4205, lr 2.40e-06, time 134ms\n",
            "iter 2660: loss 2.3095, lr 2.39e-06, time 134ms\n",
            "iter 2670: loss 2.4690, lr 2.37e-06, time 132ms\n",
            "iter 2680: loss 2.4458, lr 2.36e-06, time 133ms\n",
            "iter 2690: loss 2.3239, lr 2.34e-06, time 133ms\n",
            "iter 2700: loss 2.2689, lr 2.33e-06, time 132ms\n",
            "iter 2710: loss 2.3206, lr 2.32e-06, time 133ms\n",
            "iter 2720: loss 2.5488, lr 2.30e-06, time 133ms\n",
            "iter 2730: loss 2.5647, lr 2.29e-06, time 133ms\n",
            "iter 2740: loss 2.4682, lr 2.28e-06, time 134ms\n",
            "\n",
            "Step 2750: train loss 2.3693, val loss 2.4037\n",
            "iter 2750: loss 2.5140, lr 2.26e-06, time 2493ms\n",
            "iter 2760: loss 2.4618, lr 2.25e-06, time 133ms\n",
            "iter 2770: loss 2.3945, lr 2.23e-06, time 134ms\n",
            "iter 2780: loss 2.5337, lr 2.22e-06, time 132ms\n",
            "iter 2790: loss 2.4060, lr 2.21e-06, time 134ms\n",
            "iter 2800: loss 2.4236, lr 2.19e-06, time 133ms\n",
            "iter 2810: loss 2.4414, lr 2.18e-06, time 133ms\n",
            "iter 2820: loss 2.2651, lr 2.17e-06, time 133ms\n",
            "iter 2830: loss 2.5806, lr 2.15e-06, time 133ms\n",
            "iter 2840: loss 2.4192, lr 2.14e-06, time 134ms\n",
            "iter 2850: loss 2.4745, lr 2.13e-06, time 132ms\n",
            "iter 2860: loss 2.4083, lr 2.11e-06, time 132ms\n",
            "iter 2870: loss 2.3828, lr 2.10e-06, time 134ms\n",
            "iter 2880: loss 2.5849, lr 2.09e-06, time 134ms\n",
            "iter 2890: loss 2.3730, lr 2.07e-06, time 134ms\n",
            "iter 2900: loss 2.2779, lr 2.06e-06, time 135ms\n",
            "iter 2910: loss 2.4700, lr 2.05e-06, time 133ms\n",
            "iter 2920: loss 2.4482, lr 2.04e-06, time 134ms\n",
            "iter 2930: loss 2.3835, lr 2.02e-06, time 134ms\n",
            "iter 2940: loss 2.3431, lr 2.01e-06, time 134ms\n",
            "iter 2950: loss 2.4558, lr 2.00e-06, time 134ms\n",
            "iter 2960: loss 2.3547, lr 1.98e-06, time 133ms\n",
            "iter 2970: loss 2.4942, lr 1.97e-06, time 133ms\n",
            "iter 2980: loss 2.3067, lr 1.96e-06, time 132ms\n",
            "iter 2990: loss 2.5374, lr 1.95e-06, time 133ms\n",
            "\n",
            "Step 3000: train loss 2.3528, val loss 2.3743\n",
            "iter 3000: loss 2.5036, lr 1.93e-06, time 2500ms\n",
            "iter 3010: loss 2.2357, lr 1.92e-06, time 134ms\n",
            "iter 3020: loss 2.3810, lr 1.91e-06, time 135ms\n",
            "iter 3030: loss 2.3944, lr 1.90e-06, time 132ms\n",
            "iter 3040: loss 2.6039, lr 1.89e-06, time 134ms\n",
            "iter 3050: loss 2.3371, lr 1.87e-06, time 132ms\n",
            "iter 3060: loss 2.4082, lr 1.86e-06, time 133ms\n",
            "iter 3070: loss 2.5687, lr 1.85e-06, time 135ms\n",
            "iter 3080: loss 2.3290, lr 1.84e-06, time 132ms\n",
            "iter 3090: loss 2.3064, lr 1.82e-06, time 134ms\n",
            "iter 3100: loss 2.1976, lr 1.81e-06, time 133ms\n",
            "iter 3110: loss 2.4262, lr 1.80e-06, time 132ms\n",
            "iter 3120: loss 2.4216, lr 1.79e-06, time 134ms\n",
            "iter 3130: loss 2.4674, lr 1.78e-06, time 134ms\n",
            "iter 3140: loss 2.5822, lr 1.77e-06, time 133ms\n",
            "iter 3150: loss 2.5141, lr 1.75e-06, time 135ms\n",
            "iter 3160: loss 2.6612, lr 1.74e-06, time 132ms\n",
            "iter 3170: loss 2.3475, lr 1.73e-06, time 134ms\n",
            "iter 3180: loss 2.3982, lr 1.72e-06, time 134ms\n",
            "iter 3190: loss 2.3564, lr 1.71e-06, time 134ms\n",
            "iter 3200: loss 2.3834, lr 1.70e-06, time 131ms\n",
            "iter 3210: loss 2.4616, lr 1.69e-06, time 134ms\n",
            "iter 3220: loss 2.3367, lr 1.68e-06, time 135ms\n",
            "iter 3230: loss 2.3125, lr 1.66e-06, time 134ms\n",
            "iter 3240: loss 2.3879, lr 1.65e-06, time 135ms\n",
            "\n",
            "Step 3250: train loss 2.3441, val loss 2.3609\n",
            "üíæ Saved checkpoint (val_loss: 2.3609)\n",
            "iter 3250: loss 2.4457, lr 1.64e-06, time 3685ms\n",
            "iter 3260: loss 2.3539, lr 1.63e-06, time 133ms\n",
            "iter 3270: loss 2.4334, lr 1.62e-06, time 134ms\n",
            "iter 3280: loss 2.4491, lr 1.61e-06, time 133ms\n",
            "iter 3290: loss 2.4032, lr 1.60e-06, time 132ms\n",
            "iter 3300: loss 2.7116, lr 1.59e-06, time 134ms\n",
            "iter 3310: loss 2.5471, lr 1.58e-06, time 133ms\n",
            "iter 3320: loss 2.5174, lr 1.57e-06, time 133ms\n",
            "iter 3330: loss 2.6531, lr 1.56e-06, time 135ms\n",
            "iter 3340: loss 2.2564, lr 1.55e-06, time 132ms\n",
            "iter 3350: loss 2.5676, lr 1.54e-06, time 134ms\n",
            "iter 3360: loss 2.4135, lr 1.53e-06, time 133ms\n",
            "iter 3370: loss 2.4267, lr 1.52e-06, time 132ms\n",
            "iter 3380: loss 2.5610, lr 1.51e-06, time 134ms\n",
            "iter 3390: loss 2.3673, lr 1.50e-06, time 134ms\n",
            "iter 3400: loss 2.5182, lr 1.49e-06, time 134ms\n",
            "iter 3410: loss 2.5550, lr 1.48e-06, time 134ms\n",
            "iter 3420: loss 2.6336, lr 1.47e-06, time 134ms\n",
            "iter 3430: loss 2.4155, lr 1.46e-06, time 133ms\n",
            "iter 3440: loss 2.5717, lr 1.45e-06, time 134ms\n",
            "iter 3450: loss 2.2793, lr 1.44e-06, time 133ms\n",
            "iter 3460: loss 2.6082, lr 1.43e-06, time 134ms\n",
            "iter 3470: loss 2.4783, lr 1.42e-06, time 133ms\n",
            "iter 3480: loss 2.2745, lr 1.42e-06, time 133ms\n",
            "iter 3490: loss 2.3847, lr 1.41e-06, time 134ms\n",
            "\n",
            "Step 3500: train loss 2.3773, val loss 2.3610\n",
            "iter 3500: loss 2.4382, lr 1.40e-06, time 2477ms\n",
            "iter 3510: loss 2.4618, lr 1.39e-06, time 133ms\n",
            "iter 3520: loss 2.3883, lr 1.38e-06, time 133ms\n",
            "iter 3530: loss 2.3731, lr 1.37e-06, time 135ms\n",
            "iter 3540: loss 2.5380, lr 1.36e-06, time 134ms\n",
            "iter 3550: loss 2.5835, lr 1.35e-06, time 133ms\n",
            "iter 3560: loss 2.3876, lr 1.35e-06, time 133ms\n",
            "iter 3570: loss 2.5256, lr 1.34e-06, time 132ms\n",
            "iter 3580: loss 2.3744, lr 1.33e-06, time 133ms\n",
            "iter 3590: loss 2.2775, lr 1.32e-06, time 134ms\n",
            "iter 3600: loss 2.5964, lr 1.31e-06, time 134ms\n",
            "iter 3610: loss 2.4079, lr 1.31e-06, time 133ms\n",
            "iter 3620: loss 2.5159, lr 1.30e-06, time 136ms\n",
            "iter 3630: loss 2.6178, lr 1.29e-06, time 134ms\n",
            "iter 3640: loss 2.4920, lr 1.28e-06, time 132ms\n",
            "iter 3650: loss 2.5948, lr 1.28e-06, time 134ms\n",
            "iter 3660: loss 2.3832, lr 1.27e-06, time 133ms\n",
            "iter 3670: loss 2.7362, lr 1.26e-06, time 135ms\n",
            "iter 3680: loss 2.3370, lr 1.25e-06, time 135ms\n",
            "iter 3690: loss 2.4846, lr 1.25e-06, time 134ms\n",
            "iter 3700: loss 2.4623, lr 1.24e-06, time 135ms\n",
            "iter 3710: loss 2.3686, lr 1.23e-06, time 133ms\n",
            "iter 3720: loss 2.3869, lr 1.23e-06, time 133ms\n",
            "iter 3730: loss 2.3064, lr 1.22e-06, time 133ms\n",
            "iter 3740: loss 2.5846, lr 1.21e-06, time 133ms\n",
            "\n",
            "Step 3750: train loss 2.3407, val loss 2.3161\n",
            "üíæ Saved checkpoint (val_loss: 2.3161)\n",
            "iter 3750: loss 2.3746, lr 1.21e-06, time 3704ms\n",
            "iter 3760: loss 2.3822, lr 1.20e-06, time 132ms\n",
            "iter 3770: loss 2.1311, lr 1.19e-06, time 131ms\n",
            "iter 3780: loss 2.5881, lr 1.19e-06, time 134ms\n",
            "iter 3790: loss 2.2677, lr 1.18e-06, time 133ms\n",
            "iter 3800: loss 2.2843, lr 1.17e-06, time 135ms\n",
            "iter 3810: loss 2.4508, lr 1.17e-06, time 134ms\n",
            "iter 3820: loss 2.5206, lr 1.16e-06, time 133ms\n",
            "iter 3830: loss 2.2474, lr 1.16e-06, time 133ms\n",
            "iter 3840: loss 2.4048, lr 1.15e-06, time 132ms\n",
            "iter 3850: loss 2.3598, lr 1.15e-06, time 134ms\n",
            "iter 3860: loss 2.3188, lr 1.14e-06, time 135ms\n",
            "iter 3870: loss 2.4390, lr 1.13e-06, time 131ms\n",
            "iter 3880: loss 2.4351, lr 1.13e-06, time 132ms\n",
            "iter 3890: loss 2.4068, lr 1.12e-06, time 134ms\n",
            "iter 3900: loss 2.3602, lr 1.12e-06, time 134ms\n",
            "iter 3910: loss 2.5030, lr 1.11e-06, time 133ms\n",
            "iter 3920: loss 2.3775, lr 1.11e-06, time 134ms\n",
            "iter 3930: loss 2.5627, lr 1.10e-06, time 133ms\n",
            "iter 3940: loss 2.5409, lr 1.10e-06, time 135ms\n",
            "iter 3950: loss 2.2569, lr 1.10e-06, time 132ms\n",
            "iter 3960: loss 2.4678, lr 1.09e-06, time 132ms\n",
            "iter 3970: loss 2.3279, lr 1.09e-06, time 133ms\n",
            "iter 3980: loss 2.3090, lr 1.08e-06, time 133ms\n",
            "iter 3990: loss 2.5136, lr 1.08e-06, time 132ms\n",
            "\n",
            "Step 4000: train loss 2.2985, val loss 2.3607\n",
            "iter 4000: loss 2.4105, lr 1.07e-06, time 2500ms\n",
            "iter 4010: loss 2.4874, lr 1.07e-06, time 134ms\n",
            "iter 4020: loss 2.3582, lr 1.07e-06, time 132ms\n",
            "iter 4030: loss 2.4960, lr 1.06e-06, time 133ms\n",
            "iter 4040: loss 2.1678, lr 1.06e-06, time 134ms\n",
            "iter 4050: loss 2.4795, lr 1.06e-06, time 133ms\n",
            "iter 4060: loss 2.4581, lr 1.05e-06, time 132ms\n",
            "iter 4070: loss 2.3761, lr 1.05e-06, time 134ms\n",
            "iter 4080: loss 2.5407, lr 1.05e-06, time 132ms\n",
            "iter 4090: loss 2.5166, lr 1.04e-06, time 133ms\n",
            "iter 4100: loss 2.2753, lr 1.04e-06, time 133ms\n",
            "iter 4110: loss 2.4897, lr 1.04e-06, time 133ms\n",
            "iter 4120: loss 2.4224, lr 1.03e-06, time 132ms\n",
            "iter 4130: loss 2.3647, lr 1.03e-06, time 134ms\n",
            "iter 4140: loss 2.5398, lr 1.03e-06, time 134ms\n",
            "iter 4150: loss 2.3607, lr 1.03e-06, time 134ms\n",
            "iter 4160: loss 2.3972, lr 1.02e-06, time 135ms\n",
            "iter 4170: loss 2.3043, lr 1.02e-06, time 132ms\n",
            "iter 4180: loss 2.3799, lr 1.02e-06, time 133ms\n",
            "iter 4190: loss 2.4193, lr 1.02e-06, time 132ms\n",
            "iter 4200: loss 2.3519, lr 1.02e-06, time 132ms\n",
            "iter 4210: loss 2.4002, lr 1.01e-06, time 134ms\n",
            "iter 4220: loss 2.4790, lr 1.01e-06, time 132ms\n",
            "iter 4230: loss 2.2522, lr 1.01e-06, time 134ms\n",
            "iter 4240: loss 2.4276, lr 1.01e-06, time 135ms\n",
            "\n",
            "Step 4250: train loss 2.3352, val loss 2.3172\n",
            "iter 4250: loss 2.4374, lr 1.01e-06, time 2507ms\n",
            "iter 4260: loss 2.4490, lr 1.01e-06, time 133ms\n",
            "iter 4270: loss 2.4684, lr 1.01e-06, time 134ms\n",
            "iter 4280: loss 2.3909, lr 1.00e-06, time 133ms\n",
            "iter 4290: loss 2.3767, lr 1.00e-06, time 133ms\n",
            "iter 4300: loss 2.5336, lr 1.00e-06, time 135ms\n",
            "iter 4310: loss 2.4418, lr 1.00e-06, time 133ms\n",
            "iter 4320: loss 2.5424, lr 1.00e-06, time 133ms\n",
            "iter 4330: loss 2.3057, lr 1.00e-06, time 132ms\n",
            "iter 4340: loss 2.5986, lr 1.00e-06, time 134ms\n",
            "iter 4350: loss 2.3124, lr 1.00e-06, time 133ms\n",
            "iter 4360: loss 2.3401, lr 1.00e-06, time 134ms\n",
            "iter 4370: loss 2.5848, lr 1.00e-06, time 132ms\n",
            "\n",
            "==================================================\n",
            "‚úÖ TRAINING COMPLETE! Best val loss: 2.3161\n",
            "Model saved to: out-sft/ckpt.pt\n",
            "==================================================\n"
          ]
        }
      ],
      "id": "KRJtgjbymj0J"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yIv3Zk8Cmj0K"
      },
      "source": [
        "## 6. Test the SFT Model"
      ],
      "id": "yIv3Zk8Cmj0K"
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "mdgLrSqWmj0K",
        "outputId": "18e52c85-7dde-4862-89eb-309088a5c71d"
      },
      "source": [
        "import tiktoken\n",
        "\n",
        "checkpoint = torch.load(os.path.join(OUTPUT_DIR, 'ckpt.pt'), map_location=device)\n",
        "model_args = checkpoint['model_args']\n",
        "model_args['dropout'] = 0.0\n",
        "\n",
        "gptconf = GPTConfig(**model_args)\n",
        "model = GPT(gptconf)\n",
        "model.load_state_dict(checkpoint['model'])\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "enc = tiktoken.get_encoding('gpt2')\n",
        "\n",
        "def generate_response(prompt, max_tokens=200, temperature=0.8):\n",
        "    formatted = f\"<user>\\n{prompt}\\n</user>\\n<assistant>\\n\"\n",
        "    x = torch.tensor(enc.encode(formatted), dtype=torch.long, device=device).unsqueeze(0)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        y = model.generate(x, max_tokens, temperature=temperature, top_k=50)\n",
        "\n",
        "    response = enc.decode(y[0].tolist())\n",
        "    if \"<assistant>\" in response:\n",
        "        response = response.split(\"<assistant>\")[-1]\n",
        "    if \"</assistant>\" in response:\n",
        "        response = response.split(\"</assistant>\")[0]\n",
        "    return response.strip()\n",
        "\n",
        "print(\"‚úÖ Model ready for testing!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of parameters: 123.65M\n",
            "‚úÖ Model ready for testing!\n"
          ]
        }
      ],
      "id": "mdgLrSqWmj0K"
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "WF6e6dommj0L",
        "outputId": "1bbca756-0cf8-45e7-eef8-00973556c742"
      },
      "source": [
        "# Test with finance questions\n",
        "prompts = [\n",
        "    \"What is compound interest?\",\n",
        "    \"Explain the difference between stocks and bonds.\",\n",
        "    \"What is a 401k?\",\n",
        "]\n",
        "\n",
        "for p in prompts:\n",
        "    print(f\"\\n{'='*50}\")\n",
        "    print(f\"USER: {p}\")\n",
        "    print(f\"{'='*50}\")\n",
        "    print(f\"ASSISTANT: {generate_response(p)}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "USER: What is compound interest?\n",
            "==================================================\n",
            "ASSISTANT: The compound interest of an index is that it is the rate at which they sell or pay a price, and that is the price paid on the basis of the supply and demand for that asset.\n",
            "The ratio between compound interest and the nominal rate is 2.5, and as a result, it is the ratio of compound interest on the index that is the cost of money per unit of the index.\n",
            "Inflation and price stability are important factors in our economic decisions. We have to make sure that prices are stable, otherwise we could run out of money.\n",
            "The use of compound interest is a widely used tool in monetary policy. It is used to calculate the cost of money for governments when interest rates rise beyond the level of inflation. It has been used to calculate the cost of a bond payment and to determine the inflation rate for a particular maturity.\n",
            "The use of compound interest is also used to determine the interest rate for the price of government bonds. It is used to calculate the cost\n",
            "\n",
            "==================================================\n",
            "USER: Explain the difference between stocks and bonds.\n",
            "==================================================\n",
            "ASSISTANT: The stock market is the largest and most liquid market in the world. This market is dominated by companies that possess the market capitalization of 80% or more of a company's shares and bonds. The bonds and the stock market are primarily used for the sale of securities (e.g. debt, stocks) as well as for the purchase of assets (including dividends).\n",
            "The stock market has significant market influence because it is the single largest and most liquid market in the world, as demonstrated by the following chart: Stock prices are traded on the stock exchanges and on the exchange. By selling bonds and stock options, traders can allocate their capital to buy and sell stock. This is because of the significant market influence and the volatility of prices. The bond market, on the other hand, is largely regulated by the government, and it is not allowed to make any money off the market. This means that the bond market can continue to be dominated by companies that have the primary role of owning and managing the\n",
            "\n",
            "==================================================\n",
            "USER: What is a 401k?\n",
            "==================================================\n",
            "ASSISTANT: A 401k is a plan that pays the monthly distributions based on the income earned from your employer. (For example, if you earn $30,000 per year from your employer, you're taxed at 12% of your income.) This means that the monthly distributions should be paid each month based on your income, not based on a specific paycheck based on your salary or other information.\n",
            "One notable difference in your 401k is that the individual who owes the money on it has to contribute at a lower salary than the individual who owes the money on it. Therefore, the individual who owes a portion of the money on the money must contribute at a lower salary than the individual who owes the money on it.\n",
            "The person with the highest salary must contribute at a lower salary than the person who owes the money on the money. Additionally, a person with the highest salary can contribute more than the person with the lowest salary.\n",
            "It's important to understand that a 401k is not a 401k\n"
          ]
        }
      ],
      "id": "WF6e6dommj0L"
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "p7copZ8amj0L",
        "outputId": "6e58c417-bbc3-4d66-8370-16132de49ef5"
      },
      "source": [
        "# Ask your own question!\n",
        "prompt = \"What should I consider when investing?\"\n",
        "\n",
        "print(f\"USER: {prompt}\\n\")\n",
        "print(f\"ASSISTANT: {generate_response(prompt, max_tokens=300)}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "USER: What should I consider when investing?\n",
            "\n",
            "ASSISTANT: When it comes to investing, there are a number of things you can consider when investing. Firstly, it's important to remember that you're investing in an asset that is not yet in your portfolio. If you can't find it, there's certainly a good chance you can't find it on your own. It's also important to have some flexibility in the amount of money you invest. You can make these suggestions for each investment.\n",
            "1. Consider the asset's value and the associated cost of the investment. For example, if the asset is $500 million, you might want to consider taking an interest rate and a credit card to pay off the loan. However, you will need to consider the cost of the interest rate, credit card and even the actual cost of the investment. 2. Consider the investment's volatility. If the asset is volatile, it will likely have negative returns and you may have to invest in a different investment to find out which one you're investing in. Additionally, it's important to be aware of the fact that investing in a specific asset will not necessarily be a good investment for you. 3. Consider the risk of losing money. If the asset is volatile, you might want to make sure that it's not lost in the future. You can't afford to lose money on a volatile asset, so the risk of losing money on a volatile asset is not necessarily as bad as losing money on the same asset.\n",
            "In summary, I recommend taking some time to\n"
          ]
        }
      ],
      "id": "p7copZ8amj0L"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KLOlPCltmj0M"
      },
      "source": [
        "## 7. Download Trained Model"
      ],
      "id": "KLOlPCltmj0M"
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "OA8lbvKYmj0M"
      },
      "source": [
        "# Download the trained SFT model\n",
        "# Rename with descriptive name\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "output_name = 'out-sft_ckpt.pt'\n",
        "shutil.copy('out-sft/ckpt.pt', output_name)\n",
        "\n",
        "print(f\"‚úÖ Copied checkpoint to: {output_name}\")\n",
        "print(f\"File size: {os.path.getsize(output_name) / (1024*1024):.2f} MB\")\n",
        "\n",
        "# If on Colab, download it\n",
        "try:\n",
        "    from google.colab import files\n",
        "    files.download(output_name)\n",
        "    print(\"‚¨áÔ∏è Download started!\")\n",
        "except:\n",
        "    print(\"üíæ File ready for download via file browser\")"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "OA8lbvKYmj0M"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}